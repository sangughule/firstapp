{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..............Using Large-Scale Social Media Networks as a Scalable Sensing System for Modeling Real-Time Energy Utilization Patterns...............#\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as ske\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets from twitter app\n",
    "consumer_key = 'HJw23IyMkqZWo3GbHI9wnaPdh'\n",
    "consumer_secret = 'lX8QRL8xi45ikirHmSCyeZNaf31kXvSnjqZRn3yoL6iyjCv1OQ'\n",
    "access_key = '1219165592445308928-XCL9BBwRBHp0m6xvjZrFSRAEzylmf3'\n",
    "access_secret ='SPWUkDkM1K7TKYB81tnKayAl1XADwQzUXgpSDMTiZHl2w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authentication of keys \n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 13:45:18 b'Hey @Porsche, I can assure you: I am more than ready. Get me out on the street! \\nLove, Electricity https://t.co/TDvO8Dukcv'\n",
      "2019-07-15 15:12:21 b'Hey @Porsche, Thanks for answering my call. Can\\xe2\\x80\\x99t wait to feel again. \\nLove, Electricity #SoulElectrified https://t.co/nnFvHvkyoE'\n",
      "2019-07-12 15:29:17 b'Hey @Porsche , thanks for listening. Can\\xe2\\x80\\x99t wait to see what you have in mind. \\nLove, Electricity https://t.co/uYWc2ASMhb'\n",
      "2019-07-10 15:27:28 b'@tweetingbrit @Porsche Hey @tweetingbrit , so am I! Love, Electricity'\n",
      "2019-07-10 15:26:44 b'@tomas_sears @Porsche @Nelly_Mo Hey @tomas_sears , wanna go and take a ride wit me?'\n",
      "2019-07-10 15:25:57 b\"@KungFuCutBug @Porsche Fair point. But maybe there's another wheel I can take.\"\n",
      "2019-07-09 13:50:37 b'@jimleeke @USNavy Hey @jimleeke , so now the time has come! Love, Electricity'\n",
      "2019-07-09 13:50:11 b\"@ArglesJon @Porsche Hey @ArglesJon , after all, I power Twitter's server, so it was rather easy to convince them!\"\n",
      "2019-07-09 13:44:52 b'@Ian_1944 @Porsche Hey @Ian_1944 , thanks for the \"nice touch\"... Wait, is that a morse sending related pun?'\n",
      "2019-07-05 19:38:56 b'@ErikSdalen @Porsche Hey @ErikSdalen , and I can\\xe2\\x80\\x99t wait to join you for the ride! \\nLove, Electricity'\n",
      "2019-07-05 19:37:25 b'@lamey_jenny Hey @lamey_jenny , sadly Scranton wasn\\xe2\\x80\\x99t available. Love, Electricity'\n",
      "2019-07-05 13:15:30 b\"@ayesha_ellen Hey @ayesha_ellen, more like 'turn the lights off, then on, then off again' many times over. Love, Electricity.\"\n",
      "2019-07-05 13:14:45 b'@Fernand50948390 Si?'\n",
      "2019-07-05 13:14:20 b'@KimptonMarina Hey @KimptonMarina, thanks! I was worried my Morse was a little rusty. Love, Electricity.'\n",
      "2019-07-05 13:12:38 b\"@grogee I'm no scientist, but when you rub a balloon really hard...\"\n",
      "2019-07-05 13:11:30 b'@A_CHEEKYBOY Cheekyboy indeed. Love, Electricity.'\n",
      "2019-07-04 15:16:34 b'Hey @Porsche, get me out on the road. \\nLove, Electricity\\n#ElectricityTalks #LondonEye https://t.co/DvqoWiEIqx'\n",
      "2019-07-04 14:13:08 b\"@MikeCooper_33 Hey @MikeCooper_33, thanks! There's more to me than electric toothbrushes you know. Love, Electricity.\"\n",
      "2019-07-04 14:11:54 b'@KamungaMukanya \\xf0\\x9f\\x94\\x8d'\n",
      "2019-07-04 14:10:44 b'@maunyk Hey @maunyk, more of an RSVP. Love, Electricity.'\n",
      "2019-07-04 14:09:46 b\"@lnmanning1 Hmm, normally I don't wait for an invitation. I just kinda show up. Love, Electricity.\"\n",
      "2019-07-04 10:20:38 b'@ISRecruit \\xf0\\x9f\\x94\\x8d'\n",
      "2019-07-04 10:19:53 b'@sophiejons @TanyaBurr Time travel is a bit of a drain. But the sights are incredible. Love, Electricity.'\n",
      "2019-07-03 14:13:16 b'@short_duncan @MarcusButler @g7kse Hey @short_duncan, need to hear it again? https://t.co/0Ggqsyt8Zx. Love, Electricity.'\n",
      "2019-07-03 14:04:26 b\"@RaphvsMaximvs Hey @RaphvsMaximvs, technically speaking...I was here first. But I'm always happy to share! Love, Electricity.\"\n",
      "2019-07-03 14:03:12 b'@Anastas48835184 Is there an echo in here? Thanks for spreading the word. Love, Electricity.'\n",
      "2019-07-03 14:02:41 b'@Peter_Mugridge @SomeTellyBloke @MarcusButler My one weakness! Love, Electricity.'\n",
      "2019-07-03 14:01:51 b'@B1naryDave @TanyaBurr Hey @B1naryDave, Any interplanetary travel was purely accidental. Love, Electricity.'\n",
      "2019-07-03 14:00:28 b\"@sophiejons @TanyaBurr Hey @sophiejons, Well... Last time I checked, the #LondonEye wasn't a police box... Love, Electricity.\"\n",
      "2019-07-03 13:59:19 b'@rosariveradavid @MarcusButler Hey @rosariveradavid, I find the first three a lot more fun. Love, Electricity.'\n",
      "2019-07-03 13:57:57 b'@LewisKFlood @MarcusButler Hey @LewisKFlood, Two words: nope! Love, Electricity.'\n",
      "2019-07-03 13:55:10 b'@AmazonsArthur @MarcusButler Hey @AmazonsArthur, Agreed! Love, Electricity.'\n",
      "2019-07-03 13:54:01 b'@FrauWeiss75 @MarcusButler Hey @FrauWeiss75, As long as no one takes me to your leaders. Love, Electricity.'\n",
      "2019-07-03 13:52:51 b\"@MiniSabotage @behzbydaylight @MarcusButler Hey @MiniSabotage, quality sleuthing. Now let's hope my message is heard! Love, Electricity.\"\n",
      "2019-07-03 13:50:40 b\"@ALJOMABOB Can't wait to take it for a spin! Love, Electricity.\"\n",
      "2019-07-02 21:41:54 b'@electrichull Hey @electrichull , so I guess we share a passion for everything electrified! Thanks for helping me s\\xe2\\x80\\xa6 https://t.co/zwhKGEe1g0'\n",
      "2019-07-02 19:58:37 b'@PompeyCal @sophiepb_03 Hey @PompeyCal , I love providing tweets with power, but spreading my message in morse felt\\xe2\\x80\\xa6 https://t.co/b5EBSHMZdQ'\n",
      "2019-07-02 19:57:42 b'@_MikeArmstrong_ @LMP110894 Hey @_MikeArmstrong_ , I meant the code was nearly cracked! No need to be in London - I\\xe2\\x80\\xa6 https://t.co/GW6XC1TkBc'\n",
      "2019-07-02 19:56:27 b'@behzbydaylight @MiniSabotage @MarcusButler Hey @behzbydaylight , give decoding a try - you might be enlightened! A\\xe2\\x80\\xa6 https://t.co/c7OA6C7AiE'\n",
      "2019-07-02 19:55:38 b\"@ALJOMABOB Hey @ALJOMABOB , I'm glad you got my message. Thanks for spreading the word! Love, Electricity\"\n",
      "2019-07-02 17:02:39 b'@HappeningLDN Hey @HappeningLDN , thanks for making people aware of my message. I had a blast playing with the Eye\\xe2\\x80\\xa6 https://t.co/DpLZ5Pu2Uv'\n",
      "2019-07-02 14:28:49 b'@LMP110894 Hey @LMP110894 , thanks for listening. You nearly nailed it, but it\\xe2\\x80\\x99s not quite what I said. Help me spr\\xe2\\x80\\xa6 https://t.co/4wBulDcPBc'\n",
      "2019-07-02 14:27:37 b'@APWorsley @DrStrangetwit Hey @APWorsley , thanks for listening. You nearly nailed it, but it\\xe2\\x80\\x99s not quite what I sa\\xe2\\x80\\xa6 https://t.co/5M9s8y51IC'\n",
      "2019-07-02 12:30:00 b'@sophiepb_03 Hey @sophiepb_03 , This is my full message to the world, maybe you hear me clearer now? Love, Electric\\xe2\\x80\\xa6 https://t.co/O9jmzc5kCe'\n",
      "2019-07-02 12:29:00 b'@saschawuerker @MarcusButler Hey @saschawuerker , thank you for your help! This is my full message to the world, ma\\xe2\\x80\\xa6 https://t.co/dpp5q2MWqT'\n",
      "2019-07-02 12:24:55 b'@kaialtenfelder @MarcusButler Hey @kaialtenfelder , thank you for your help! This is my full message to the world,\\xe2\\x80\\xa6 https://t.co/4BXmwZurfl'\n",
      "2019-07-02 12:23:03 b'@DavidSDevlin Hey @DavidSDevlin , thank you for trying to help me. This is my full message to the world, maybe you\\xe2\\x80\\xa6 https://t.co/W7cFEfGIDR'\n",
      "2019-07-02 12:20:52 b'@sunseeker32 @TheLondonEye Hey @sunseeker32 , thank you for trying to help me. This is my full message to the world\\xe2\\x80\\xa6 https://t.co/KsC7wQ2QUM'\n",
      "2019-07-02 12:19:35 b'@jakedavisphoto @MarcusButler Hey @jakedavisphoto , thank you for trying to help me. This is my full message to the\\xe2\\x80\\xa6 https://t.co/Qq3w48Wkgg'\n",
      "2019-07-02 12:17:58 b'@MiniSabotage @behzbydaylight @MarcusButler Hey @behzbydaylight &amp; @MiniSabotage , thank you for trying to help me.\\xe2\\x80\\xa6 https://t.co/U5tSmtREOC'\n",
      "2019-07-02 12:15:11 b'@EvaReckhard @Porsche Hey @EvaReckhard , thank you for trying to help me. This is my full message to the world, may\\xe2\\x80\\xa6 https://t.co/6T9bj2GTJn'\n",
      "2019-07-02 12:13:12 b'@KathrynLewis10 @danrubin Hey @KathrynLewis10 , thank you for trying to help me. This is my full message to the wor\\xe2\\x80\\xa6 https://t.co/MauLRgvxkT'\n",
      "2019-07-02 12:02:01 b'@ThatGuyThere02 @PorscheGB Hey @ThatGuyThere02, I\\xe2\\x80\\x99m glad you got my message. Thank you for spreading the word to th\\xe2\\x80\\xa6 https://t.co/ddkiEQqMTd'\n",
      "2019-07-01 17:53:30 b'Hey London, Hey World: Did you get my message? \\nLove, Electricity \\n#LondonEye #DecodeTheEye #ElectricityTalks'\n",
      "2019-07-01 10:18:35 b'RT @ttboyshine: Is the #londoneye trying to communicate with us tonight #london #SadiqKhan #morsecode'\n",
      "2019-07-01 10:17:21 b'RT @sunseeker32: Anyone know, why the London eye has been flashing blue ??? #londoneye #london  @TheLondonEye https://t.co/YgRhc64dxv'\n",
      "2019-07-01 10:15:39 b'RT @danrubin: Here\\xe2\\x80\\x99s a video I filmed during my live stream last night at the #LondonEye \\xe2\\x80\\x94 can you figure out what @electricity was telling\\xe2\\x80\\xa6'\n",
      "2019-07-01 10:14:58 b'RT @TanyaBurr: AD | What has happened to the London Eye? Can you help me figure it out? #DecodeTheEye #ElectricityTalks https://t.co/LJTrfy\\xe2\\x80\\xa6'\n",
      "2019-07-01 09:25:51 b'RT @TanyaBurr: Help me crack what @Electricity is saying on the #LondonEye... https://t.co/ynjb2fwp7N'\n",
      "2019-07-01 09:25:27 b'RT @danrubin: I\\xe2\\x80\\x99m in London tonight, across from the #LondonEye and something strange is happening with the lights\\xe2\\x80\\xa6 help me figure it out?\\xe2\\x80\\xa6'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-30 20:30:01 b'Knock, knock - is this thing on? Find me at the #LondonEye now. #ElectricityTalks'\n",
      "2019-06-30 20:00:03 b\"Half an hour... I'm a bit nervous. #ElectricityTalks\"\n",
      "2019-06-30 13:00:11 b\"Hey #London, I'm preparing my speech. Look forward to meeting you at the #LondonEye at 9.30pm. Love, Electricity. #ElectricityTalks\"\n",
      "2019-06-30 09:37:35 b'Hey #London, there is something I have to say. Meet me at the #LondonEye at 9.30pm. Love, Electricity. #ElectricityTalks'\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "for tweet in tweepy.Cursor(api.user_timeline, \n",
    "                           id = \"Electricity\",\n",
    "                           since='2011-03-20',until='2011-03-25').items(100): \n",
    "    print(tweet.created_at, tweet.text.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import tweepy\n",
    "csvFile = open('tweets.csv', 'a')\n",
    "\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "for tweet in tweepy.Cursor(api.user_timeline, \n",
    "                           id = \"Electricity\",\n",
    "                           since='2018-03-20',until='2018-03-25').items(100): \n",
    "    csvWriter.writerow([tweet.text.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('tweets.csv',names=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Hey @Porsche, I can assure you: I am more th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Hey @Porsche, Thanks for answering my call. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Hey @Porsche , thanks for listening. Can\\xe2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'@tweetingbrit @Porsche Hey @tweetingbrit , s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'@tomas_sears @Porsche @Nelly_Mo Hey @tomas_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b\"@KungFuCutBug @Porsche Fair point. But maybe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b'@jimleeke @USNavy Hey @jimleeke , so now the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b\"@ArglesJon @Porsche Hey @ArglesJon , after a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'@Ian_1944 @Porsche Hey @Ian_1944 , thanks fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'@ErikSdalen @Porsche Hey @ErikSdalen , and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b'@lamey_jenny Hey @lamey_jenny , sadly Scrant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b\"@ayesha_ellen Hey @ayesha_ellen, more like '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b'@Fernand50948390 Si?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b'@KimptonMarina Hey @KimptonMarina, thanks! I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b\"@grogee I'm no scientist, but when you rub a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b'@A_CHEEKYBOY Cheekyboy indeed. Love, Electri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b'Hey @Porsche, get me out on the road. \\nLove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b\"@MikeCooper_33 Hey @MikeCooper_33, thanks! T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>b'@KamungaMukanya \\xf0\\x9f\\x94\\x8d'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>b'@maunyk Hey @maunyk, more of an RSVP. Love, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>b\"@lnmanning1 Hmm, normally I don't wait for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b'@ISRecruit \\xf0\\x9f\\x94\\x8d'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>b'@sophiejons @TanyaBurr Time travel is a bit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>b'@short_duncan @MarcusButler @g7kse Hey @shor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>b\"@RaphvsMaximvs Hey @RaphvsMaximvs, technical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>b'@Anastas48835184 Is there an echo in here? T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>b'@Peter_Mugridge @SomeTellyBloke @MarcusButle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>b'@B1naryDave @TanyaBurr Hey @B1naryDave, Any ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>b\"@sophiejons @TanyaBurr Hey @sophiejons, Well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b'@rosariveradavid @MarcusButler Hey @rosarive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>b'RT @danrubin: I\\xe2\\x80\\x99m in London tonig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>b'Knock, knock - is this thing on? Find me at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>b\"Half an hour... I'm a bit nervous. #Electric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>b\"Hey #London, I'm preparing my speech. Look f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>b'Hey #London, there is something I have to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>b'RT @tsknray: List of items affected for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>b'List of items affected for common people in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweets\n",
       "0   b'Hey @Porsche, I can assure you: I am more th...\n",
       "1   b'Hey @Porsche, Thanks for answering my call. ...\n",
       "2   b'Hey @Porsche , thanks for listening. Can\\xe2...\n",
       "3   b'@tweetingbrit @Porsche Hey @tweetingbrit , s...\n",
       "4   b'@tomas_sears @Porsche @Nelly_Mo Hey @tomas_s...\n",
       "5   b\"@KungFuCutBug @Porsche Fair point. But maybe...\n",
       "6   b'@jimleeke @USNavy Hey @jimleeke , so now the...\n",
       "7   b\"@ArglesJon @Porsche Hey @ArglesJon , after a...\n",
       "8   b'@Ian_1944 @Porsche Hey @Ian_1944 , thanks fo...\n",
       "9   b'@ErikSdalen @Porsche Hey @ErikSdalen , and I...\n",
       "10  b'@lamey_jenny Hey @lamey_jenny , sadly Scrant...\n",
       "11  b\"@ayesha_ellen Hey @ayesha_ellen, more like '...\n",
       "12                            b'@Fernand50948390 Si?'\n",
       "13  b'@KimptonMarina Hey @KimptonMarina, thanks! I...\n",
       "14  b\"@grogee I'm no scientist, but when you rub a...\n",
       "15  b'@A_CHEEKYBOY Cheekyboy indeed. Love, Electri...\n",
       "16  b'Hey @Porsche, get me out on the road. \\nLove...\n",
       "17  b\"@MikeCooper_33 Hey @MikeCooper_33, thanks! T...\n",
       "18                b'@KamungaMukanya \\xf0\\x9f\\x94\\x8d'\n",
       "19  b'@maunyk Hey @maunyk, more of an RSVP. Love, ...\n",
       "20  b\"@lnmanning1 Hmm, normally I don't wait for a...\n",
       "21                     b'@ISRecruit \\xf0\\x9f\\x94\\x8d'\n",
       "22  b'@sophiejons @TanyaBurr Time travel is a bit ...\n",
       "23  b'@short_duncan @MarcusButler @g7kse Hey @shor...\n",
       "24  b\"@RaphvsMaximvs Hey @RaphvsMaximvs, technical...\n",
       "25  b'@Anastas48835184 Is there an echo in here? T...\n",
       "26  b'@Peter_Mugridge @SomeTellyBloke @MarcusButle...\n",
       "27  b'@B1naryDave @TanyaBurr Hey @B1naryDave, Any ...\n",
       "28  b\"@sophiejons @TanyaBurr Hey @sophiejons, Well...\n",
       "29  b'@rosariveradavid @MarcusButler Hey @rosarive...\n",
       "..                                                ...\n",
       "59  b'RT @danrubin: I\\xe2\\x80\\x99m in London tonig...\n",
       "60  b'Knock, knock - is this thing on? Find me at ...\n",
       "61  b\"Half an hour... I'm a bit nervous. #Electric...\n",
       "62  b\"Hey #London, I'm preparing my speech. Look f...\n",
       "63  b'Hey #London, there is something I have to sa...\n",
       "64  b'RT @tsknray: List of items affected for comm...\n",
       "65  b'RT @tsknray: List of items affected for comm...\n",
       "66  b'RT @tsknray: List of items affected for comm...\n",
       "67  b'RT @tsknray: List of items affected for comm...\n",
       "68  b'RT @tsknray: List of items affected for comm...\n",
       "69  b'RT @tsknray: List of items affected for comm...\n",
       "70  b'RT @tsknray: List of items affected for comm...\n",
       "71  b'RT @tsknray: List of items affected for comm...\n",
       "72  b'RT @tsknray: List of items affected for comm...\n",
       "73  b'RT @tsknray: List of items affected for comm...\n",
       "74  b'RT @tsknray: List of items affected for comm...\n",
       "75  b'RT @tsknray: List of items affected for comm...\n",
       "76  b'RT @tsknray: List of items affected for comm...\n",
       "77  b'RT @tsknray: List of items affected for comm...\n",
       "78  b'RT @tsknray: List of items affected for comm...\n",
       "79  b'RT @tsknray: List of items affected for comm...\n",
       "80  b'RT @tsknray: List of items affected for comm...\n",
       "81  b'RT @tsknray: List of items affected for comm...\n",
       "82  b'RT @tsknray: List of items affected for comm...\n",
       "83  b'RT @tsknray: List of items affected for comm...\n",
       "84  b'RT @tsknray: List of items affected for comm...\n",
       "85  b'RT @tsknray: List of items affected for comm...\n",
       "86  b'RT @tsknray: List of items affected for comm...\n",
       "87  b'RT @tsknray: List of items affected for comm...\n",
       "88  b'List of items affected for common people in ...\n",
       "\n",
       "[89 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting all tweets to the collected_tweets csv file\n",
    "import csv\n",
    "import tweepy\n",
    "csvFile = open('tweets.csv', 'a')\n",
    "\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,q='Electricity',\n",
    "                           lang=\"en\",\n",
    "                           since=1-1-2019).items(100):\n",
    "    csvWriter.writerow([tweet.text.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('tweets.csv',names=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Hey @Porsche, I can assure you: I am more th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Hey @Porsche, Thanks for answering my call. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Hey @Porsche , thanks for listening. Can\\xe2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'@tweetingbrit @Porsche Hey @tweetingbrit , s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'@tomas_sears @Porsche @Nelly_Mo Hey @tomas_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b\"@KungFuCutBug @Porsche Fair point. But maybe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b'@jimleeke @USNavy Hey @jimleeke , so now the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b\"@ArglesJon @Porsche Hey @ArglesJon , after a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'@Ian_1944 @Porsche Hey @Ian_1944 , thanks fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'@ErikSdalen @Porsche Hey @ErikSdalen , and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b'@lamey_jenny Hey @lamey_jenny , sadly Scrant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b\"@ayesha_ellen Hey @ayesha_ellen, more like '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b'@Fernand50948390 Si?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b'@KimptonMarina Hey @KimptonMarina, thanks! I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b\"@grogee I'm no scientist, but when you rub a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b'@A_CHEEKYBOY Cheekyboy indeed. Love, Electri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b'Hey @Porsche, get me out on the road. \\nLove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b\"@MikeCooper_33 Hey @MikeCooper_33, thanks! T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>b'@KamungaMukanya \\xf0\\x9f\\x94\\x8d'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>b'@maunyk Hey @maunyk, more of an RSVP. Love, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>b\"@lnmanning1 Hmm, normally I don't wait for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b'@ISRecruit \\xf0\\x9f\\x94\\x8d'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>b'@sophiejons @TanyaBurr Time travel is a bit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>b'@short_duncan @MarcusButler @g7kse Hey @shor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>b\"@RaphvsMaximvs Hey @RaphvsMaximvs, technical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>b'@Anastas48835184 Is there an echo in here? T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>b'@Peter_Mugridge @SomeTellyBloke @MarcusButle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>b'@B1naryDave @TanyaBurr Hey @B1naryDave, Any ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>b\"@sophiejons @TanyaBurr Hey @sophiejons, Well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b'@rosariveradavid @MarcusButler Hey @rosarive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>b\"@ALJOMABOB Can't wait to take it for a spin!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>b'@electrichull Hey @electrichull , so I guess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>b'@PompeyCal @sophiepb_03 Hey @PompeyCal , I l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>b'@_MikeArmstrong_ @LMP110894 Hey @_MikeArmstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>b'@behzbydaylight @MiniSabotage @MarcusButler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>b\"@ALJOMABOB Hey @ALJOMABOB , I'm glad you got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>b'@HappeningLDN Hey @HappeningLDN , thanks for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>b'@LMP110894 Hey @LMP110894 , thanks for liste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>b'@APWorsley @DrStrangetwit Hey @APWorsley , t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>b'@sophiepb_03 Hey @sophiepb_03 , This is my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>b'@saschawuerker @MarcusButler Hey @saschawuer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>b'@kaialtenfelder @MarcusButler Hey @kaialtenf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>b'@DavidSDevlin Hey @DavidSDevlin , thank you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>b'@sunseeker32 @TheLondonEye Hey @sunseeker32 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>b'@jakedavisphoto @MarcusButler Hey @jakedavis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>b'@MiniSabotage @behzbydaylight @MarcusButler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>b'@EvaReckhard @Porsche Hey @EvaReckhard , tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>b'@KathrynLewis10 @danrubin Hey @KathrynLewis1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>b'@ThatGuyThere02 @PorscheGB Hey @ThatGuyThere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>b'Hey London, Hey World: Did you get my messag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>b'RT @ttboyshine: Is the #londoneye trying to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>b'RT @sunseeker32: Anyone know, why the London...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>b'RT @danrubin: Here\\xe2\\x80\\x99s a video I fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>b'RT @TanyaBurr: AD | What has happened to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>b'RT @TanyaBurr: Help me crack what @Electrici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>b'RT @danrubin: I\\xe2\\x80\\x99m in London tonig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>b'Knock, knock - is this thing on? Find me at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>b\"Half an hour... I'm a bit nervous. #Electric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>b\"Hey #London, I'm preparing my speech. Look f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>b'Hey #London, there is something I have to sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets\n",
       "0    b'Hey @Porsche, I can assure you: I am more th...\n",
       "1    b'Hey @Porsche, Thanks for answering my call. ...\n",
       "2    b'Hey @Porsche , thanks for listening. Can\\xe2...\n",
       "3    b'@tweetingbrit @Porsche Hey @tweetingbrit , s...\n",
       "4    b'@tomas_sears @Porsche @Nelly_Mo Hey @tomas_s...\n",
       "5    b\"@KungFuCutBug @Porsche Fair point. But maybe...\n",
       "6    b'@jimleeke @USNavy Hey @jimleeke , so now the...\n",
       "7    b\"@ArglesJon @Porsche Hey @ArglesJon , after a...\n",
       "8    b'@Ian_1944 @Porsche Hey @Ian_1944 , thanks fo...\n",
       "9    b'@ErikSdalen @Porsche Hey @ErikSdalen , and I...\n",
       "10   b'@lamey_jenny Hey @lamey_jenny , sadly Scrant...\n",
       "11   b\"@ayesha_ellen Hey @ayesha_ellen, more like '...\n",
       "12                             b'@Fernand50948390 Si?'\n",
       "13   b'@KimptonMarina Hey @KimptonMarina, thanks! I...\n",
       "14   b\"@grogee I'm no scientist, but when you rub a...\n",
       "15   b'@A_CHEEKYBOY Cheekyboy indeed. Love, Electri...\n",
       "16   b'Hey @Porsche, get me out on the road. \\nLove...\n",
       "17   b\"@MikeCooper_33 Hey @MikeCooper_33, thanks! T...\n",
       "18                 b'@KamungaMukanya \\xf0\\x9f\\x94\\x8d'\n",
       "19   b'@maunyk Hey @maunyk, more of an RSVP. Love, ...\n",
       "20   b\"@lnmanning1 Hmm, normally I don't wait for a...\n",
       "21                      b'@ISRecruit \\xf0\\x9f\\x94\\x8d'\n",
       "22   b'@sophiejons @TanyaBurr Time travel is a bit ...\n",
       "23   b'@short_duncan @MarcusButler @g7kse Hey @shor...\n",
       "24   b\"@RaphvsMaximvs Hey @RaphvsMaximvs, technical...\n",
       "25   b'@Anastas48835184 Is there an echo in here? T...\n",
       "26   b'@Peter_Mugridge @SomeTellyBloke @MarcusButle...\n",
       "27   b'@B1naryDave @TanyaBurr Hey @B1naryDave, Any ...\n",
       "28   b\"@sophiejons @TanyaBurr Hey @sophiejons, Well...\n",
       "29   b'@rosariveradavid @MarcusButler Hey @rosarive...\n",
       "..                                                 ...\n",
       "123  b\"@ALJOMABOB Can't wait to take it for a spin!...\n",
       "124  b'@electrichull Hey @electrichull , so I guess...\n",
       "125  b'@PompeyCal @sophiepb_03 Hey @PompeyCal , I l...\n",
       "126  b'@_MikeArmstrong_ @LMP110894 Hey @_MikeArmstr...\n",
       "127  b'@behzbydaylight @MiniSabotage @MarcusButler ...\n",
       "128  b\"@ALJOMABOB Hey @ALJOMABOB , I'm glad you got...\n",
       "129  b'@HappeningLDN Hey @HappeningLDN , thanks for...\n",
       "130  b'@LMP110894 Hey @LMP110894 , thanks for liste...\n",
       "131  b'@APWorsley @DrStrangetwit Hey @APWorsley , t...\n",
       "132  b'@sophiepb_03 Hey @sophiepb_03 , This is my f...\n",
       "133  b'@saschawuerker @MarcusButler Hey @saschawuer...\n",
       "134  b'@kaialtenfelder @MarcusButler Hey @kaialtenf...\n",
       "135  b'@DavidSDevlin Hey @DavidSDevlin , thank you ...\n",
       "136  b'@sunseeker32 @TheLondonEye Hey @sunseeker32 ...\n",
       "137  b'@jakedavisphoto @MarcusButler Hey @jakedavis...\n",
       "138  b'@MiniSabotage @behzbydaylight @MarcusButler ...\n",
       "139  b'@EvaReckhard @Porsche Hey @EvaReckhard , tha...\n",
       "140  b'@KathrynLewis10 @danrubin Hey @KathrynLewis1...\n",
       "141  b'@ThatGuyThere02 @PorscheGB Hey @ThatGuyThere...\n",
       "142  b'Hey London, Hey World: Did you get my messag...\n",
       "143  b'RT @ttboyshine: Is the #londoneye trying to ...\n",
       "144  b'RT @sunseeker32: Anyone know, why the London...\n",
       "145  b'RT @danrubin: Here\\xe2\\x80\\x99s a video I fi...\n",
       "146  b'RT @TanyaBurr: AD | What has happened to the...\n",
       "147  b'RT @TanyaBurr: Help me crack what @Electrici...\n",
       "148  b'RT @danrubin: I\\xe2\\x80\\x99m in London tonig...\n",
       "149  b'Knock, knock - is this thing on? Find me at ...\n",
       "150  b\"Half an hour... I'm a bit nervous. #Electric...\n",
       "151  b\"Hey #London, I'm preparing my speech. Look f...\n",
       "152  b'Hey #London, there is something I have to sa...\n",
       "\n",
       "[153 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 1: Preprocessing Steps for Social Media Network Data:\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from termcolor import colored\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import datasets\n",
    "print(\"Loading data\")\n",
    "tweets_data = pd.read_csv('tweets.csv',names=['tweets'])\n",
    "\n",
    "\n",
    "# Setting stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Function to expand tweet\n",
    "def expand_tweet(tweet):\n",
    "\texpanded_tweet = []\n",
    "\tfor word in tweet:\n",
    "\t\tif re.search(\"n't\", word):\n",
    "\t\t\texpanded_tweet.append(word.split(\"n't\")[0])\n",
    "\t\t\texpanded_tweet.append(\"not\")\n",
    "\t\telse:\n",
    "\t\t\texpanded_tweet.append(word)\n",
    "\treturn expanded_tweet\n",
    "\n",
    "# Function to process tweets\n",
    "def clean_tweet(data, wordNetLemmatizer, porterStemmer):\n",
    "\tdata['Clean_tweet'] = data['tweets']\n",
    "\tprint(colored(\"Removing user handles starting with @\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].str.replace(\"@[\\w]*\",\"\")\n",
    "\tprint(colored(\"Removing numbers and special characters\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].str.replace(\"[^a-zA-Z' ]\",\"\")\n",
    "\tprint(colored(\"Removing urls\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"((www\\.[^\\s]+)|(https?://[^\\s]+))\"), \"\")\n",
    "\tprint(colored(\"Removing single characters\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"(^| ).( |$)\"), \"\")\n",
    "\tprint(colored(\"Tokenizing\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].str.split()\n",
    "\tprint(colored(\"Removing stopwords\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [word for word in tweet if word not in STOPWORDS])\n",
    "\tprint(colored(\"Expanding not words\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: expand_tweet(tweet))\n",
    "\tprint(colored(\"Lemmatizing the words\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [wordNetLemmatizer.lemmatize(word) for word in tweet])\n",
    "\tprint(colored(\"Stemming the words\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [porterStemmer.stem(word) for word in tweet])\n",
    "\tprint(colored(\"Combining words back to tweets\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: ' '.join(tweet))\n",
    "\treturn data\n",
    "\n",
    "# Define processing methods\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "porterStemmer = PorterStemmer()\n",
    "\n",
    "# Pre-processing the tweets\n",
    "print(colored(\"Processing data\", \"green\"))\n",
    "tweets_data = clean_tweet(tweets_data, wordNetLemmatizer, porterStemmer)\n",
    "tweets_data.to_csv('clean_tweets.csv', index = False)\n",
    "print(colored(\"Tweets data processed and saved to clean_tweets.csv\", \"green\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word vector processing:\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "  \n",
    "#  Reads ‘alice.txt’ file \n",
    "sample = open(\"tweets.csv\", \"r\") \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") \n",
    "  \n",
    "data = [] \n",
    "  \n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "  \n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'profitable' \" + \n",
    "               \"and 'majority' - CBOW : \", \n",
    "    model1.similarity('profitable', 'majority')) \n",
    "\n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 1) \n",
    "\n",
    "  \n",
    "      \n",
    "print(\"Cosine similarity between 'profitable' \" +\n",
    "            \"and 'majority' - Skip Gram : \", \n",
    "      model2.similarity('profitable', 'majority')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "reviews = [row for row in csv.reader(open('clean_tweets.csv'))]\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def process_text(text):\n",
    "    # Make all the strings lowercase and remove non alphabetic characters\n",
    "    text = re.sub('[^A-Za-z]', ' ', text.lower())\n",
    "\n",
    "    # Tokenize the text; this is, separate every sentence into a list of words\n",
    "    # Since the text is already split into sentences you don't have to call sent_tokenize\n",
    "    tokenized_text = word_tokenize(text)\n",
    "\n",
    "    # Remove the stopwords and stem each word to its root\n",
    "    clean_text = [\n",
    "        stemmer.stem(word) for word in tokenized_text\n",
    "        if word not in stopwords.words('english')\n",
    "    ]\n",
    "\n",
    "    # Remember, this final output is a list of words\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first row, since it only has the labels\n",
    "reviews = reviews[1:]\n",
    "\n",
    "texts = [row[1] for row in reviews]\n",
    "topics = [row[1] for row in reviews]\n",
    "\n",
    "# Process the texts to so they are ready for training\n",
    "# But transform the list of words back to string format to feed it to sklearn\n",
    "texts = [\" \".join(process_text(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "matrix = CountVectorizer(max_features=1000)\n",
    "vectors = matrix.fit_transform(texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "vectors_train, vectors_test, topics_train, topics_test = train_test_split(vectors,topics,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:0.2f}% in training set\".format((len(vectors_train)/len(reviews)) * 100))\n",
    "print(\"{0:0.2f}% in test set\".format((len(vectors_test)/len(reviews)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(vectors_train)\n",
    "X_test = sc.transform(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components=1)\n",
    "X_train = lda.fit_transform(X_train, topics_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised Machine Learning:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "classifier.fit(X_train, topics_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(vectors_train, topics_train)\n",
    "\n",
    "# Predict with the testing set\n",
    "topics_pred = classifier.predict(vectors_test)\n",
    "\n",
    "# ...and measure the accuracy of the results\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(topics_test, topics_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(topics_test, topics_pred)\n",
    "print(cm)\n",
    "print('Accuracy ' + str(accuracy_score(topics_test, topics_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using five algorithm of Enhancement like(KNN,GB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(vectors_train, topics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn.predict(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('K-nearest Neighbors (KNN):')\n",
    "print('\\n')\n",
    "print(confusion_matrix(topics_test, pred))\n",
    "print(classification_report(topics_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "import sklearn.ensemble as ske"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB = ske.GradientBoostingClassifier(n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB.fit(vectors_train, topics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_predictions = GB.predict(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting (GB):')\n",
    "print('\\n')\n",
    "print(confusion_matrix(topics_test,GB_predictions))\n",
    "print(classification_report(topics_test,GB_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_data_clean = pd.read_csv('clean_tweets.csv') \n",
    "LDA_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper function\n",
    "def plot_10_most_likely_common_words_inthe_topic(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most likely common words inthe topic')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(LDA_data_clean['Clean_tweet'])\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_likely_common_words_inthe_topic(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2: LDA Algorithm in the Context of the Proposed Social Media Network Model(Existing System)\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "#Load the LDA model from sk-learn with unsupervised Machine Learning algorithm:\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below (use int values below 15)\n",
    "number_topics = 20\n",
    "number_words = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating fit and LDA Model:\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(count_data)\n",
    "print(lda.fit(count_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LatentDirichletAllocation(LDA):\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "texts = [process_text(text) for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "#Latent Dirichlet Allocation (LDA)\n",
    "#num_topics=3\n",
    "#model = models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
    "#num_topics=20\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary, passes=15)\n",
    "print(model)\n",
    "topics = model.print_topics(num_words=3)\n",
    "# print(topics)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_model_tfidf = gensim.models.LdaMulticore(corpus, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus, num_topics=20, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(model[corpus[100]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing 20 topics:\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(model, corpus,dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[corpus[100]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nLDA Topics: {}\".format(score, lda_model_tfidf.print_topic(index, 80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing algorithm:3\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "def Unique_words_inthe_topic(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:20]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    print('Find the 20 words inthe clean tweet:')\n",
    "    print(words)\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(LDA_data_clean['Clean_tweet'])\n",
    "\n",
    "# 10 most unique words\n",
    "Unique_words_inthe_topic(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import pandas as pd \n",
    "  \n",
    "# Input the file  \n",
    "txt1 = [] \n",
    "with open('clean_tweets.csv') as file: \n",
    "    txt1 = file.readlines() \n",
    "\n",
    "\n",
    "def remove_string_special_characters(s): \n",
    "      \n",
    "    # removes special characters with ' ' \n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s) \n",
    "    stripped = re.sub('_', '', stripped) \n",
    "      \n",
    "    # Change any white space to one space \n",
    "    stripped = re.sub('\\s+', ' ', stripped) \n",
    "      \n",
    "    # Remove start and end white spaces \n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '': \n",
    "            return stripped.lower() \n",
    "          \n",
    "# Stopword removal  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "# from algorithm2(LDA Topic)\n",
    "your_list = ['love', 'hey', 'electricity', 'electr', 'help', 'messag', 'heythank', 'londoney', 'london', 'tri', 'thi', 'nlove', 'rt', 'wait', 'thank', 'world', 'spread', 'word', 'nearli', 'electricitytalk']\n",
    "for i, line in enumerate(txt1): \n",
    "    txt1[i] = ' '.join([x for \n",
    "        x in nltk.word_tokenize(line) if \n",
    "        ( x not in stop_words ) and ( x not in your_list )]) \n",
    "      \n",
    "# Getting trigrams  \n",
    "vectorizer = CountVectorizer(ngram_range = (3,3)) \n",
    "X1 = vectorizer.fit_transform(txt1)  \n",
    "features = (vectorizer.get_feature_names()) \n",
    "#print(\"\\n\\nFeatures : \\n\", features) \n",
    "print(\"\\n\\nX1 : \\n\", X1.toarray()) \n",
    "  \n",
    "# Applying TFIDF \n",
    "vectorizer = TfidfVectorizer(ngram_range = (3,3)) \n",
    "X2 = vectorizer.fit_transform(txt1) \n",
    "scores = (X2.toarray()) \n",
    "print(\"\\n\\nScores : \\n\", scores) \n",
    "  \n",
    "# Getting top rating features \n",
    "sums = X2.sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, most_likely_words_in_the_topic in enumerate(features): \n",
    "    data1.append( (most_likely_words_in_the_topic, sums[0,col] )) \n",
    "rate = pd.DataFrame(data1, columns = ['most_likely_words_in_the_topic','r-rate']) \n",
    "words = (rate.sort_values('r-rate', ascending = False)) \n",
    "\n",
    "print (\"\\n\\n topics head : \\n\", words)#.head(50)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_of_o_in_d = words['r-rate'] #o=topics, d = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_of_o_in_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.to_csv('rate_of_topics.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate=pd.read_csv('rate_of_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('most likely words in the topic')\n",
    "#plt.plot(df_rate['most_likely_words_in_the_topic'])\n",
    "#plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cor_df = pd.read_csv('clean_tweets.csv')\n",
    "#cor_df['Clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts.to_csv('correlation_of_topics.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate['most_likely_words_in_the_topic']=df_rate['most_likely_words_in_the_topic'].astype('category').cat.codes\n",
    "df_rate.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pearson correlation\n",
    "df_rate.corr(method ='pearson') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "series = read_csv('clean_tweets.csv', header=0, index_col=0)\n",
    "counts = series[\"Clean_tweet\"].value_counts()\n",
    "plot_acf(counts)\n",
    "pyplot.show()\n",
    "plot_pacf(counts, lags=50)\n",
    "pyplot.show()\n",
    "print(\"CT_cor:\",counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('clean_tweets.csv')\n",
    "print('%i subjects and %i columns' % df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "dict = {'Topic':['love', 'hey', 'electricity', 'electr', 'help', 'messag', 'heythank', 'londoney', 'london', 'tri', 'thi', 'nlove', 'rt', 'wait', 'thank', 'world', 'spread', 'word', 'nearli', 'electricitytalk',], }\n",
    "df1 = pd.DataFrame(dict) \n",
    "print(df1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_corr=pd.read_csv('rate_of_topics_corr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topics_analysis = df_rate_corr[['Date','r-rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topics_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_corr['most_likely_words_in_the_topic']=df_rate_corr['Topics'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_corr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import statsmodels.tsa.stattools as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Existing System:\n",
    "#Algorithm 3: Mapping Topics to Effects(Result: Granger Causal Topics)\n",
    "#Algorithm3:Result: Granger Causal Topics: and Algorithm 4: Computational Complexity of This Methodology:(output: Predictions)\n",
    "st.grangercausalitytests(df_rate_corr[['most_likely_words_in_the_topic', 'r-rate']], maxlag=1)\n",
    "st.grangercausalitytests(df_rate_corr[['most_likely_words_in_the_topic', 'r-rate']], maxlag=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import seaborn as sb\n",
    "sb.set_style('darkgrid')\n",
    "\n",
    "Topics_analysis['Date'] = Topics_analysis['Date']\n",
    "Topics_analysis = Topics_analysis.sort_index(by='Date')\n",
    "Topics_analysis = Topics_analysis.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topics_analysis['r-rate'].plot(figsize=(16, 12))\n",
    "plt.title('hey electricity electr electricitytalk(Topics)')\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('rate',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note:\n",
    "#The event’s time series can be compared to the document time series related to the real-world phenomena through crosscorrelation (see Algorithm 3). \n",
    "#That is, by matching events frequencies and real world phenomena by their time, can we find any relations between the two variables? This is defined\n",
    "#by the Pearson’s rank correlation where each point is a pairing of event frequencies and real world phenomena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Electricdata= pd.read_csv('SDGE-ELEC-2020-Q1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Electricdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Electricdata['CustomerClass']=Electricdata['CustomerClass'].astype('category').cat.codes\n",
    "Electricdata['Combined']=Electricdata['Combined'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Electricdata.corrwith(df_rate_corr.most_likely_words_in_the_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 5: Topics to Predictions\n",
    "#Result: Measurement of Predictive value of Social Media Network data\n",
    "\n",
    "import seaborn as sns\n",
    "sns.factorplot(x='Month',y='AveragekWh',data=Electricdata,kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "sns.kdeplot(Electricdata['AveragekWh'])\n",
    "#plot(Rate)\n",
    "#plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.rugplot(Electricdata['AveragekWh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(Electricdata['AveragekWh'])\n",
    "sns.rugplot(Electricdata['AveragekWh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
